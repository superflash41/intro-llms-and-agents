{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9PwqB7rb1Cg"
      },
      "source": [
        "# Homework 1\n",
        "\n",
        "*   Goals:\n",
        "  * Implement Cross-Entropy loss\n",
        "  * Implement SDPA (Scaled-dot product attention)\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8lKYBqNkO16"
      },
      "source": [
        "# Cross-Entropy Computation\n",
        "\n",
        "Given a target vector *y_true* and a prediction vector *y_pred* (produced by a model), write a function that computes the **Cross-Entropy Loss** between both.\n",
        "\n",
        "The formula for the cross-entropy loss is:\n",
        "\n",
        "$H(t, q) = - \\sum_{x \\in X}\\:  t(x) \\: log \\: p(x) $\n",
        "\n",
        "- *t* is the **true probability** distribution\n",
        "- *p* is the **predicted probability** distribution\n",
        "- *X* is the **set of classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1FmMZsm9hS78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8edfbf1-4688-4382-b981-4ab46b609719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Entropy Loss:  0.010199795719758164\n",
            "Success!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "y_true = [1, 0, 0, 0, 0]\n",
        "y_pred = [10, 5, 3, 1, 4]\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    cross-entropy implementation\n",
        "    \"\"\"\n",
        "    # calculate the softmax of y_pred\n",
        "    exp = np.exp(y_pred)\n",
        "    sum_exp = np.sum(exp)\n",
        "    softmax = exp / sum_exp\n",
        "    # compute the cross-entropy according to the formula\n",
        "    ce = -np.sum(y_true * np.log(softmax))\n",
        "\n",
        "    return ce\n",
        "\n",
        "\n",
        "loss = cross_entropy(y_true, y_pred)\n",
        "\n",
        "print(\"Cross Entropy Loss: \", loss)\n",
        "assert(loss == 0.010199795719758164)\n",
        "print(\"Success!\") # nice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhaCecoCTjjn"
      },
      "source": [
        "# Scaled Dot-Product Attention\n",
        "\n",
        "***The task is to implement a simple version of the scaled-dot product attention mechanism.***\n",
        "\n",
        "I will use `pytorch`!\n",
        "\n",
        "My implementation will:\n",
        "*   take $3$ tensors with dimensions $[batch\\_size, seq\\_len, d_{model}]$,\n",
        "*   compute attention using the scaled dot-product attention formula,\n",
        "*   and return a tensor with the same dimensions as the input.\n",
        "\n",
        "---\n",
        "\n",
        "## Reference\n",
        "> [Vaswani et. al (2017)](https://arxiv.org/pdf/1706.03762) is the famous paper that introduced the Transformer architecture.\n",
        "\n",
        "> The most important concept is the *scaled dot-product attention*. Using  matrices *query*, *key*, and *value*  $Q, K, M$, this mechanism is defined as\\\n",
        "$\\text{Attention}(Q,K,V)=\\sigma(\\frac{QK^T}{\\sqrt{d_k}})V$\\\n",
        "where $d_k$ is the dimensionality of the row $K$ (commonly in practice, $d_k = d_q = d_v = d_{model}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWWUAVpjT7Bo",
        "outputId": "e6f98e60-8185-4e02-e72c-5a582e210611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import softmax, einsum # the suggestion of using einsum\n",
        "\n",
        "def sdp_attention(q, k, v):\n",
        "  \"\"\"\n",
        "  scaled dot-product attention implementation\n",
        "  \"\"\"\n",
        "  # for each batch b we compute the dot product between\n",
        "  # each i in the query vector and each j in the key vector\n",
        "  scores = einsum(\"bid, bjd -> bij\", q, k)\n",
        "  # scale by sqrt(d_k)\n",
        "  d_k = q.shape[-1]\n",
        "  scores /= d_k ** 0.5\n",
        "  # apply softmax\n",
        "  probs = softmax(scores, dim=-1)\n",
        "  # multiply by V\n",
        "  score = einsum(\"bij, bjd -> bid\", probs, v)\n",
        "\n",
        "  return score\n",
        "\n",
        "### TEST CASE ###\n",
        "from torch import manual_seed, randn\n",
        "manual_seed(42)\n",
        "\n",
        "q,k,v = [randn(1,4,8) for _ in range(3)]\n",
        "outp = sdp_attention(q, k, v)\n",
        "expected = torch.tensor([[[0.26668164134025574, 0.2370503693819046, -0.05542190372943878, 0.12984780967235565, 0.354112833738327, -0.19060277938842773, -0.6448009014129639, -0.008517783135175705], [0.10862354934215546, 0.2443515509366989, -0.2163696587085724, 0.38144612312316895, 0.06310948729515076, -0.5632890462875366, -1.1007437705993652, -0.33056533336639404], [0.49469706416130066, -0.10946226119995117, -0.5350303649902344, 0.3420145511627197, -0.62238609790802, -0.47719380259513855, 0.322252094745636, 0.2334655523300171], [0.5705238580703735, -0.014622762799263, -0.27748897671699524, 0.32384753227233887, -0.46800583600997925, -0.4084155559539795, 0.19807007908821106, 0.32957538962364197]]])\n",
        "assert torch.allclose(expected, outp.data, atol=0.0001)\n",
        "print(\"Success!\") # nice!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}