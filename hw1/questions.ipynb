{"cells":[{"cell_type":"markdown","metadata":{"id":"s9PwqB7rb1Cg"},"source":["# Trabajo de Laboratorio 1\n","\n","*   Objetivo:\n","  * Implementar Cross-Entropy loss\n","  * Implementar SDPA (Scaled-dot product attention)\n","\n","\n","---\n","---\n","---"]},{"cell_type":"markdown","metadata":{"id":"l8lKYBqNkO16"},"source":["# Cross Entropy Computation\n","\n","Dado un vector target *y_true* y un vector de prediccion *y_pred* (tal como es producido por el modelo), escribe una funcion que calcule el Cross-Entropy loss entre ambos.\n","\n","$H(t, q) = - \\sum_{x \\in X}\\:  t(x) \\: log \\: p(x) $\n","\n","donde *t* es la verdadera distribucion de probabilidad, *p* es la distribucion de probabilidad predecida, y *X* es el grupo de clases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1FmMZsm9hS78"},"outputs":[],"source":["import numpy as np\n","\n","y_true = [1, 0, 0, 0, 0]\n","y_pred = [10, 5, 3, 1, 4]\n","\n","def cross_entropy(y_true, y_pred):\n","    '''\n","    YOUR CODE HERE\n","    '''\n","\n","loss = cross_entropy(y_true, y_pred)\n","\n","print(\"Cross Entropy Loss: \", loss)\n","assert(loss == 0.010199795719758164)\n","print(\"Success!\")"]},{"cell_type":"markdown","metadata":{"id":"AhaCecoCTjjn"},"source":["# Scaled Dot-Product Attention\n","\n","***Su tarea es implementar una version simple del mecanismo de atencion scaled-dot-product.***\n","\n","Puede usar cualquier framework o libreria que prefiera.\n","\n","Su implementacion debe:\n","*   tomar $3$ tensores de dimensiones $[batch\\_size, seq\\_len, d_{model}]$,\n","*   calcular atencion usando el mecanismo de scaled dot-product attention como es descrito debajo,\n","*   y retornar un tensor con la misma dimension.\n","\n","Para esta tarea, no es necesario considerar otros conceptos como multi-head attention o attention masks (causal attention).\n","\n","---\n","\n","> [Vaswani et. al (2017)](https://arxiv.org/pdf/1706.03762) es el articulo que introdujo la arquitectura Transformer.\n","\n","> El concepto mas importante es el de *scaled dot-product attention*. Usando  matrices *query*, *key*, y *value*  $Q, K, M$, este mecanismo se define como\\\n","$\\text{Attention}(Q,K,V)=\\sigma(\\frac{QK^T}{\\sqrt{d_k}})V$\\\n","donde $d_k$ es la dimensionalidad de fila de $K$ (tipicamente en practica, $d_k = d_q = d_v = d_{model}$)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1741698180250,"user":{"displayName":"Gerasimos Lampouras","userId":"04598019676479211194"},"user_tz":0},"id":"rWWUAVpjT7Bo","outputId":"c0f06169-6d31-4ef5-8c9e-60fbeb0c669e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Success!\n"]}],"source":["\n","import torch\n","from torch import softmax, einsum\n","\n","def sdp_attention(q, k, v):\n","  \"\"\"\n","  YOUR CODE HERE\n","  \"\"\"\n","\n","  return score\n","\n","### TEST CASE ###\n","from torch import manual_seed, randn\n","manual_seed(42)\n","\n","q,k,v = [randn(1,4,8) for _ in range(3)]\n","outp = sdp_attention(q, k, v)\n","expected = torch.tensor([[[0.26668164134025574, 0.2370503693819046, -0.05542190372943878, 0.12984780967235565, 0.354112833738327, -0.19060277938842773, -0.6448009014129639, -0.008517783135175705], [0.10862354934215546, 0.2443515509366989, -0.2163696587085724, 0.38144612312316895, 0.06310948729515076, -0.5632890462875366, -1.1007437705993652, -0.33056533336639404], [0.49469706416130066, -0.10946226119995117, -0.5350303649902344, 0.3420145511627197, -0.62238609790802, -0.47719380259513855, 0.322252094745636, 0.2334655523300171], [0.5705238580703735, -0.014622762799263, -0.27748897671699524, 0.32384753227233887, -0.46800583600997925, -0.4084155559539795, 0.19807007908821106, 0.32957538962364197]]])\n","assert torch.allclose(expected, outp.data, atol=0.0001)\n","print(\"Success!\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1lNteLRMHUf1507sqquDsGbNefw0q92P3","timestamp":1742138032138},{"file_id":"1rKiJbNsaRnZNvJg9q5QDseZPLIe3BDGb","timestamp":1713879445117}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}